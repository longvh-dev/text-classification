{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8144627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in /Users/long/anaconda3/lib/python3.11/site-packages (6.8.0)\n",
      "Requirement already satisfied: Click>=6.0 in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (8.0.4)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (0.9.9)\n",
      "Requirement already satisfied: nltk in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (2.31.0)\n",
      "Requirement already satisfied: joblib in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (1.3.0)\n",
      "Requirement already satisfied: PyYAML in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (6.0)\n",
      "Requirement already satisfied: underthesea-core==1.0.4 in /Users/long/anaconda3/lib/python3.11/site-packages (from underthesea) (1.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/long/anaconda3/lib/python3.11/site-packages (from nltk->underthesea) (2023.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/long/anaconda3/lib/python3.11/site-packages (from requests->underthesea) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/long/anaconda3/lib/python3.11/site-packages (from requests->underthesea) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/long/anaconda3/lib/python3.11/site-packages (from requests->underthesea) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/long/anaconda3/lib/python3.11/site-packages (from requests->underthesea) (2023.7.22)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/long/anaconda3/lib/python3.11/site-packages (from scikit-learn->underthesea) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/long/anaconda3/lib/python3.11/site-packages (from scikit-learn->underthesea) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/long/anaconda3/lib/python3.11/site-packages (from scikit-learn->underthesea) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3d02ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b00f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14976\n",
      "14976\n",
      "['__label__Ghế_văn_phòng', '__label__Thước_dây', '__label__Cưa_tay', '__label__Váy,_đầm', '__label__Dụng_cụ_đo,_kiểm_tra_khác']\n",
      "['Ghế HQ - HK095', '30m Thước dây làm bằng sợi thủy tinh TOTAL TMTF12306', 'Cưa gỗ cầm tay cán lớn Asaki AK - 8657', 'Đầm ren cổ V tay ngắn cao cấp', 'Máy đo độ đồng tâm HANN YAN 6401D1']\n"
     ]
    }
   ],
   "source": [
    "# read txt file\n",
    "f = open(\"input/product_train_0.txt\", \"r\")\n",
    "\n",
    "# get category and product\n",
    "categories = []\n",
    "products = []\n",
    "\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    category = line.split(\" \", 1)[0]\n",
    "    categories.append(category)\n",
    "\n",
    "    product = ''.join(line.split(\" \", 1)[1:]).replace(\"\\n\", \"\")\n",
    "    products.append(product)\n",
    "\n",
    "# remove duplicate\n",
    "unique_categories = list(dict.fromkeys(categories))\n",
    "\n",
    "print(len(categories))\n",
    "print(len(products))\n",
    "\n",
    "print(categories[0:5])\n",
    "print(products[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf80cb4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            products  \\\n",
      "0                                     Ghế HQ - HK095   \n",
      "1  30m Thước dây làm bằng sợi thủy tinh TOTAL TMT...   \n",
      "2             Cưa gỗ cầm tay cán lớn Asaki AK - 8657   \n",
      "3                      Đầm ren cổ V tay ngắn cao cấp   \n",
      "4                 Máy đo độ đồng tâm HANN YAN 6401D1   \n",
      "\n",
      "                           categories  \n",
      "0              __label__Ghế_văn_phòng  \n",
      "1                  __label__Thước_dây  \n",
      "2                    __label__Cưa_tay  \n",
      "3                   __label__Váy,_đầm  \n",
      "4  __label__Dụng_cụ_đo,_kiểm_tra_khác  \n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(list(zip(products, categories)),\n",
    "                    columns=['products', 'categories'])\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cb6fe",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff38268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "products      0\n",
       "categories    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check misssing values\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0f0177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14976, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbfd8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories\n",
      "__label__Bao_đựng,_ốp_lưng_điện_thoại    342\n",
      "__label__Bàn_ghế_phòng_khách             187\n",
      "__label__Bộ_bàn_ghế_cafe                 175\n",
      "__label__Váy,_đầm                        158\n",
      "__label__Bộ_bàn_ghế_(_phòng_ăn,_bếp_)    135\n",
      "                                        ... \n",
      "__label__Tời_kéo                           1\n",
      "__label__Van_công_nghiệp_khác              1\n",
      "__label__Máy_in_khổ_lớn                    1\n",
      "__label__Bể_cá                             1\n",
      "__label__Bánh_pía                          1\n",
      "Name: count, Length: 1971, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check target balance\n",
    "\n",
    "print(data['categories'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e28a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcbb7e89",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236590f4",
   "metadata": {},
   "source": [
    "## 1. Chuẩn hoá unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa1ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2444beb",
   "metadata": {},
   "source": [
    "## 2. Chuẩn hoá kiểu gõ dấu\n",
    "\n",
    "VD: **òa** và **oà** là 2 từ khác nhau. <br>\n",
    "Chi tiết: [Wikipedia](https://vi.wikipedia.org/wiki/Quy_t%E1%BA%AFc_%C4%91%E1%BA%B7t_d%E1%BA%A5u_thanh_c%E1%BB%A7a_ch%E1%BB%AF_Qu%E1%BB%91c_ng%E1%BB%AF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f8c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh hòa thụy, đang làm.. gì\n"
     ]
    }
   ],
   "source": [
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^p{P}*)([p{L}.]*p{L}+)(p{P}*$)',\n",
    "                    r'1/2/3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    End section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
    "    Xem tại đây: https://vi.wikipedia.org/wiki/Quy_tắc_đặt_dấu_thanh_trong_chữ_quốc_ngữ\n",
    "\"\"\"\n",
    "# if __name__ == '__main__':\n",
    "print(chuan_hoa_dau_cau_tieng_viet('anh hòa thụy, đang làm.. gì'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30463bb1",
   "metadata": {},
   "source": [
    "## 3. Tách từ\n",
    "\n",
    "Học sinh học sinh học => Học_sinh học sinh_học\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e271af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tôi là sinh_viên trường đại_học bách_khoa hà_nội\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "sentence = \"Tôi là sinh viên trường đại học bách khoa hà nội\"\n",
    "print(word_tokenize(sentence, format=\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc2d52",
   "metadata": {},
   "source": [
    "## 4. Đưa về chữ thường\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368952f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tôi là sinh viên trường đại học bách khoa hà nội\n"
     ]
    }
   ],
   "source": [
    "sentence=sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc62c7",
   "metadata": {},
   "source": [
    "# Hàm tiền xử lý\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908581a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp hcm phạt người không đeo khẩu_trang nơi công_cộng người_dân ở thành_phố không đeo khẩu_trang nơi công_cộng sẽ bị xử_phạt mức cao nhất 300.000 đồng , từ ngày 58 . yêu_cầu này được chủ_tịch ubnd thành_phố nguyễn_thành_phong đưa ra tại cuộc họp ban chỉ_đạo phòng_chống dịch_bệnh covid-19 của tp hcm chiều 38 . việc xử_phạt không đeo khẩu_trang nơi công_cộng được tp hcm cũng như các địa_phương khác thực_hiện từ cuối tháng 3 khi covid-19 bùng_phát . tuy_nhiên , sau khi hết thực_hiện cách_ly xã_hội từ ngày 234 , việc đeo khẩu_trang nơi công_cộng chỉ dừng lại ở mức khuyến_cáo . theo nghị_định số 1762013 , người_dân không đeo khẩu_trang nơi công_cộng sẽ bị xử_phạt từ 100.000 đến 300.000 đồng . trong khoảng một tháng áp_dụng trước đó , tp hcm đã xử_phạt hơn 4.300 trường_hợp với gần 870 triệu đồng . theo ông phong , việc đeo khẩu_trang đã được khẳng_định có_thể tránh lây_lan dịch_bệnh cho người khác và bảo_vệ sức_khỏe cho người sử_dụng . \" sở công_thương phải nắm nguồn cung_ứng khẩu_trang , chủ_động thông_báo các điểm bán để người_dân dễ_dàng mua vì đã xử_phạt thì phải bảo_đảm đủ nguồn cung \" , ông phong nói . đội trật_tự đô_thị phường bến_nghé , quận 1 , xử_phạt người không đeo khẩu_trang trên phố đi bộ nguyễn_huệ , chiều 154 . ảnh : quỳnh_trần . đội trật_tự đô_thị phường bến_nghé , quận 1 , xử_phạt người không đeo khẩu_trang trên phố đi bộ nguyễn_huệ , chiều 154 . ảnh : quỳnh_trần . bí_thư thành ủy nguyễn_thiện_nhân cũng cho rằng việc đeo khẩu_trang là một trong những biện_pháp cơ_bản để tránh dịch_bệnh lây_lan . việc này rất dễ làm , không tốn nhiều tiền nhưng nhiều nước bỏ lơi và đã bị \" vỡ trận \" . \" ngoài đường hiện có ít_nhất 20 % người không đeo khẩu_trang . người không đeo không_những tự rước bệnh vào mình mà_còn nguy_cơ lây cho người khác . đeo khẩu_trang hơi cực tí thôi nhưng đi đâu cũng_nên đeo để giữ an_toàn \" , ông nhân nói và khẳng_định thành_phố bảo_đảm không thiếu khẩu_trang cho người_dân . chủ_tịch ubnd thành_phố nguyễn_thành_phong cũng cho biết đã đồng_ý việc tái_lập các chốt kiểm_soát ở cửa_ngõ tp hcm để phòng_chống covid-19 . trước đó , thành_phố đã lập 62 chốt kiểm_soát , hoạt_động 2424 từ ngày 44 để phòng_chống dịch . lực_lượng tham_gia là công_an thành_phố , sở y_tế , bộ_tư_lệnh thành_phố , thanh_tra giao_thông , ban quản_lý an_toàn thực_phẩm , quản_lý thị_trường . trong đó , 16 chốt chính ( cấp thành_phố ) đặt tại : trạm thu phí long_phước ( cao_tốc tp hcm - long thành - dầu_giây ) , cao_tốc trung_lương , cầu đôi ( đường trần_văn giàu ) , đường ba làng , đường xuyên á ( quốc_lộ 22 ) , cầu_phú_cường , cầu vĩnh_bình , cầu_vượt sóng_thần , quốc_lộ 1 k , quốc_lộ 50 , quốc_lộ 1 a , cầu đồng_nai , bến_xe miền tây , bến_xe miền đông , sân_bay tân_sơn nhất , cảng cát lái . đến ngày 234 , chính_quyền thành_phố dừng hoạt_động các chốt này vì dịch_bệnh đã được khống_chế , tp hcm dừng cách_ly xã_hội theo chỉ_thị 19 của thủ_tướng . sau 19 ngày hoạt_động , các chốt chính đã kiểm_tra gần 270.000 xe , trong đó có 235.000 ôtô ; gần 600.000 người được kiểm_tra y_tế , đo thân_nhiệt , bao_gồm cả 3.000 người nước_ngoài ; hơn 130.000 người được yêu_cầu khai_báo y_tế .\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "def text_preprocess(document):\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    # document = re.sub(\n",
    "    #     r'[^áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "documents = \"\"\"\n",
    "TP HCM phạt người không đeo khẩu trang nơi công cộng\n",
    "Người dân ở thành phố không đeo khẩu trang nơi công cộng sẽ bị xử phạt mức cao nhất 300.000 đồng, từ ngày 5/8.\n",
    "\n",
    "Yêu cầu này được Chủ tịch UBND thành phố Nguyễn Thành Phong đưa ra tại cuộc họp Ban chỉ đạo phòng chống dịch bệnh Covid-19 của TP HCM chiều 3/8.\n",
    "\n",
    "Việc xử phạt không đeo khẩu trang nơi công cộng được TP HCM cũng như các địa phương khác thực hiện từ cuối tháng 3 khi Covid-19 bùng phát. Tuy nhiên, sau khi hết thực hiện cách ly xã hội từ ngày 23/4, việc đeo khẩu trang nơi công cộng chỉ dừng lại ở mức khuyến cáo.\n",
    "\n",
    "Theo Nghị định số 176/2013, người dân không đeo khẩu trang nơi công cộng sẽ bị xử phạt từ 100.000 đến 300.000 đồng. Trong khoảng một tháng áp dụng trước đó, TP HCM đã xử phạt hơn 4.300 trường hợp với gần 870 triệu đồng.\n",
    "\n",
    "Theo ông Phong, việc đeo khẩu trang đã được khẳng định có thể tránh lây lan dịch bệnh cho người khác và bảo vệ sức khỏe cho người sử dụng. \"Sở Công thương phải nắm nguồn cung ứng khẩu trang, chủ động thông báo các điểm bán để người dân dễ dàng mua vì đã xử phạt thì phải bảo đảm đủ nguồn cung\", ông Phong nói.\n",
    "\n",
    "Đội trật tự đô thị phường Bến Nghé, quận 1, xử phạt người không đeo khẩu trang trên phố đi bộ Nguyễn Huệ, chiều 15/4. Ảnh: Quỳnh Trần.\n",
    "Đội trật tự đô thị phường Bến Nghé, quận 1, xử phạt người không đeo khẩu trang trên phố đi bộ Nguyễn Huệ, chiều 15/4. Ảnh: Quỳnh Trần.\n",
    "\n",
    "Bí thư Thành uỷ Nguyễn Thiện Nhân cũng cho rằng việc đeo khẩu trang là một trong những biện pháp cơ bản để tránh dịch bệnh lây lan. Việc này rất dễ làm, không tốn nhiều tiền nhưng nhiều nước bỏ lơi và đã bị \"vỡ trận\".\n",
    "\n",
    "\"Ngoài đường hiện có ít nhất 20% người không đeo khẩu trang. Người không đeo không những tự rước bệnh vào mình mà còn nguy cơ lây cho người khác. Đeo khẩu trang hơi cực tí thôi nhưng đi đâu cũng nên đeo để giữ an toàn\", ông Nhân nói và khẳng định thành phố bảo đảm không thiếu khẩu trang cho người dân.\n",
    "\n",
    "Chủ tịch UBND thành phố Nguyễn Thành Phong cũng cho biết đã đồng ý việc tái lập các chốt kiểm soát ở cửa ngõ TP HCM để phòng chống Covid-19.\n",
    "\n",
    "Trước đó, thành phố đã lập 62 chốt kiểm soát, hoạt động 24/24 từ ngày 4/4 để phòng chống dịch. Lực lượng tham gia là Công an thành phố, Sở Y tế, Bộ Tư lệnh thành phố, Thanh tra giao thông, Ban Quản lý An toàn thực phẩm, quản lý thị trường.\n",
    "\n",
    "Trong đó, 16 chốt chính (cấp thành phố) đặt tại: Trạm thu phí Long Phước (cao tốc TP HCM - Long Thành - Dầu Giây), cao tốc Trung Lương, cầu Đôi (đường Trần Văn Giàu), đường Ba Làng, đường Xuyên Á (quốc lộ 22), cầu Phú Cường, cầu Vĩnh Bình, cầu vượt Sóng Thần, quốc lộ 1K, quốc lộ 50, quốc lộ 1A, cầu Đồng Nai, Bến xe Miền Tây, Bến xe miền Đông, sân bay Tân Sơn Nhất, cảng Cát Lái.\n",
    "\n",
    "Đến ngày 23/4, chính quyền thành phố dừng hoạt động các chốt này vì dịch bệnh đã được khống chế, TP HCM dừng cách ly xã hội theo Chỉ thị 19 của Thủ tướng.\n",
    "\n",
    "Sau 19 ngày hoạt động, các chốt chính đã kiểm tra gần 270.000 xe, trong đó có 235.000 ôtô; gần 600.000 người được kiểm tra y tế, đo thân nhiệt, bao gồm cả 3.000 người nước ngoài; hơn 130.000 người được yêu cầu khai báo y tế.\n",
    "\"\"\"\n",
    "print(text_preprocess(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dee7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ghế hq - hk095', '30 m thước_dây làm bằng sợi thủy tinh_total tmtf12306', 'cưa gỗ cầm tay cán lớn asaki ak - 8657', 'đầm ren cổ v tay ngắn cao_cấp', 'máy đo độ đồng_tâm hann yan 6401 d1']\n"
     ]
    }
   ],
   "source": [
    "# process all products\n",
    "products = [text_preprocess(product) for product in products]\n",
    "print(products[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9321c5",
   "metadata": {},
   "source": [
    "## loại bỏ stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679383c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa2e1443",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình phân loại\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5ff47",
   "metadata": {},
   "source": [
    "## 1. Xây dựng tập train / test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c53bcf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__Access_point_(Wifi)', '__label__Ampe_kìm', '__label__Amplifier', '__label__Amplifier_Hi-End', '__label__An_toàn_cho_bé_khi_đi_ô_tô'] n\n",
      "['kính mắt goldsun korea gs217004 s3 59 - 12 - 145', 'decal nhôm xi_bạc', 'bàn uống nước thông_minh 003', 'bàn học_sinh có kệ sách , ngang 80 cm màu nâu , ghế tăng chỉnh_chân đơn hồng', 'bàn phím giả cơ_lightning pr - 8801']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "test_percent = 0.2\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(products, categories, test_size=test_percent,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# save train/test data\n",
    "with open('data/train.txt', 'w') as f:\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        y = y.replace(\" \", \"_\")\n",
    "        f.write('__label__' + y + ' ' + x + '\\n')\n",
    "\n",
    "with open('data/test.txt', 'w') as f:\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        y = y.replace(\" \", \"_\")\n",
    "        f.write('__label__' + y + ' ' + x + '\\n')\n",
    "\n",
    "# encode target\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['categories'])\n",
    "\n",
    "print(list(label_encoder.classes_)[0:5], 'n')\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64b3f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_PATH = \"models\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb901f46",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c1113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training Naive Bayes in 1.11991286277771 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1),\n",
    "                                              max_df=0.8,\n",
    "                                              max_features=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                     ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6efe1e",
   "metadata": {},
   "source": [
    "## 3. Linear Classifier\n",
    "\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b7938e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1),\n",
    "                                              max_df=0.8,\n",
    "                                              max_features=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression(solver='lbfgs',\n",
    "                                                multi_class='auto',\n",
    "                                                max_iter=10000))\n",
    "                     ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(\n",
    "    MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d4bf7",
   "metadata": {},
   "source": [
    "## 4. SVM\n",
    "[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c200e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training SVM in 47.48678517341614 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "start_time = time.time()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 1),\n",
    "                                              max_df=0.8,\n",
    "                                              max_features=None)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(gamma='scale'))\n",
    "                     ])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print('Done training SVM in', train_time, 'seconds.')\n",
    "\n",
    "# Save model\n",
    "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"svm.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb4dfb",
   "metadata": {},
   "source": [
    "## So sánh các mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f36b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Accuracy = 0.22997329773030709\n",
      "SVM, Accuracy = 0.503004005340454\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Naive Bayes\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
    "\n",
    "# SVM\n",
    "model = pickle.load(open(os.path.join(MODEL_PATH,\"svm.pkl\"), 'rb'))\n",
    "y_pred = model.predict(X_test)\n",
    "print('SVM, Accuracy =', np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83e1aefe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1028, does not match size of target_names, 1971. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/long/Desktop/KPDL_HLV/Text_Classification.ipynb Cell 36\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/long/Desktop/KPDL_HLV/Text_Classification.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nb_model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(MODEL_PATH,\u001b[39m\"\u001b[39m\u001b[39mnaive_bayes.pkl\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/long/Desktop/KPDL_HLV/Text_Classification.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred \u001b[39m=\u001b[39m nb_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/long/Desktop/KPDL_HLV/Text_Classification.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred, target_names\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(label_encoder\u001b[39m.\u001b[39mclasses_)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2561\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2555\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2556\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabels size, \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, does not match size of target_names, \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2557\u001b[0m                 \u001b[39mlen\u001b[39m(labels), \u001b[39mlen\u001b[39m(target_names)\n\u001b[1;32m   2558\u001b[0m             )\n\u001b[1;32m   2559\u001b[0m         )\n\u001b[1;32m   2560\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2561\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2562\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNumber of classes, \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, does not match size of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2563\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtarget_names, \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m. Try specifying the labels \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2564\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mparameter\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(labels), \u001b[39mlen\u001b[39m(target_names))\n\u001b[1;32m   2565\u001b[0m         )\n\u001b[1;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m target_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     target_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 1028, does not match size of target_names, 1971. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# Xem kết quả trên từng nhãn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
    "y_pred = nb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
